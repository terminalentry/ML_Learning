{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878423c4",
   "metadata": {},
   "source": [
    "# 📘 Notebook 2: Derive Linear Regression from First Principles\n",
    "In this notebook, we derive linear regression step-by-step from first principles — using no black boxes. We build intuition, develop the math, and connect it to real-world modeling.\n",
    "\n",
    "## Goals\n",
    "- Understand what linear regression is really doing\n",
    "- Derive the closed-form solution\n",
    "- Visualize the geometry of projection\n",
    "- Implement regression from scratch, explain every function used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724dafa",
   "metadata": {},
   "source": [
    "## ✍️ Step 1: Simulate Simple Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data: y = 3x + noise\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "true_coef = 3\n",
    "noise = np.random.normal(0, 2, X.shape[0])\n",
    "y = true_coef * X.flatten() + noise\n",
    "\n",
    "# Visualize\n",
    "px.scatter(x=X.flatten(), y=y, labels={'x':'x', 'y':'y'}, title='Simulated Linear Data with Noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a1c5b",
   "metadata": {},
   "source": [
    "## 📐 Step 2: Define the Model and Loss Function\n",
    "We assume a model of the form:\n",
    "$$ y = X\\beta + \\varepsilon $$\n",
    "Where:\n",
    "- $X$ is the matrix of input features\n",
    "- $\\beta$ is the vector of parameters\n",
    "- $\\varepsilon$ is the error term\n",
    "\n",
    "We define our loss function as the sum of squared residuals:\n",
    "$$ L(\\beta) = \\|y - X\\beta\\|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0e9cd",
   "metadata": {},
   "source": [
    "## 🧮 Step 3: Derive the Closed Form Solution\n",
    "To minimize $L(\\beta)$, we take the derivative and set it to zero:\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\|y - X\\beta\\|^2 = -2X^T(y - X\\beta) = 0 $$\n",
    "Solving:\n",
    "$$ X^TX\\beta = X^Ty $$\n",
    "Assuming $X^TX$ is invertible:\n",
    "$$ \\beta = (X^TX)^{-1}X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a6085",
   "metadata": {},
   "source": [
    "## 🧪 Step 4: Implement Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_closed_form(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Solves for beta using the normal equation.\"\"\"\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add bias term manually\n",
    "    beta_hat = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    return beta_hat\n",
    "\n",
    "beta = linear_regression_closed_form(X, y)\n",
    "print(f\"Estimated coefficients: Intercept = {beta[0]:.2f}, Slope = {beta[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9447db4",
   "metadata": {},
   "source": [
    "## 📊 Step 5: Visualize Predictions vs Reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "y_pred = X_b @ beta\n",
    "\n",
    "# Plot\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X.flatten(), y=y, mode='markers', name='Observed'))\n",
    "fig.add_trace(go.Scatter(x=X.flatten(), y=y_pred, mode='lines', name='Predicted'))\n",
    "fig.update_layout(title='Observed vs Predicted', xaxis_title='x', yaxis_title='y')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
