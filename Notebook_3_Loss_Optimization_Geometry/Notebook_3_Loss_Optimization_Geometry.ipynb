{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5f227f",
   "metadata": {},
   "source": [
    "# üìò Notebook 3: Loss, Optimization, and Geometry\n",
    "In this notebook, we go beyond the closed-form solution and investigate how optimization works.\n",
    "\n",
    "We focus on:\n",
    "- The role of the loss function in learning\n",
    "- The geometry of least squares\n",
    "- Gradient Descent as an optimization method\n",
    "- Visualizing the loss surface and convergence\n",
    "- Deriving gradients manually, step by step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0c82f",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Define the Loss Function Explicitly\n",
    "We start with the squared loss:\n",
    "$$ L(\\beta) = \\frac{1}{2} \\| y - X\\beta \\|^2 $$\n",
    "The factor $\\frac{1}{2}$ is for mathematical convenience when taking derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6a5b0",
   "metadata": {},
   "source": [
    "## üîç Step 2: Visualize the Loss Surface\n",
    "We fix intercept at zero for simplicity and visualize how the loss changes with slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(1)\n",
    "X = np.linspace(0, 5, 30).reshape(-1, 1)\n",
    "true_beta = 2.5\n",
    "y = true_beta * X.flatten() + np.random.normal(0, 1, X.shape[0])\n",
    "\n",
    "# Try different beta values\n",
    "betas = np.linspace(0, 5, 100)\n",
    "losses = []\n",
    "for b in betas:\n",
    "    y_pred = X.flatten() * b\n",
    "    loss = 0.5 * np.mean((y - y_pred)**2)\n",
    "    losses.append(loss)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=betas, y=losses, mode='lines', name='Loss'))\n",
    "fig.update_layout(title='Loss vs Beta (Slope)', xaxis_title='Beta', yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eda9a1",
   "metadata": {},
   "source": [
    "## üßÆ Step 3: Derive the Gradient\n",
    "We compute the gradient of the loss function:\n",
    "$$ \\nabla_\\beta L = -X^T(y - X\\beta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4f755",
   "metadata": {},
   "source": [
    "## üîÅ Step 4: Implement Gradient Descent by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr=0.01, epochs=100):\n",
    "    n = X.shape[0]\n",
    "    X_b = np.c_[np.ones((n, 1)), X]  # Add bias\n",
    "    beta = np.zeros((2,))\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X_b @ beta\n",
    "        error = y - y_pred\n",
    "        gradient = -X_b.T @ error / n\n",
    "        beta -= lr * gradient\n",
    "        loss = 0.5 * np.mean(error ** 2)\n",
    "        history.append((beta.copy(), loss))\n",
    "    return beta, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8d029",
   "metadata": {},
   "source": [
    "## üìà Step 5: Track Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b842e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_gd, history = gradient_descent(X, y)\n",
    "betas_list = [b[0] for b in history]\n",
    "losses = [b[1] for b in history]\n",
    "\n",
    "import plotly.express as px\n",
    "px.line(y=losses, labels={'x':'Epoch', 'y':'Loss'}, title='Loss Convergence During Gradient Descent')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
